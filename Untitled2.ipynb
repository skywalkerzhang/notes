{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "926d81bf5ac24f17bd84d7d6f954b006": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f191ced3ad0e420daabe7d1310eadc50",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b80d46f9e6ca44dab30b882afdb6b5f5",
              "IPY_MODEL_46dfa8c0970a41e3ad02c405c28a3822",
              "IPY_MODEL_e4684b9b9a424941a5c83df21d6e1c41"
            ]
          }
        },
        "f191ced3ad0e420daabe7d1310eadc50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b80d46f9e6ca44dab30b882afdb6b5f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_119a8b3205304f48a8df621273e8ee1a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_73578e285ef64143b34077c0d02a3699"
          }
        },
        "46dfa8c0970a41e3ad02c405c28a3822": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9d0b0d4e7b5441858bf99294e07ff31c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 3,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 3,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a61f981195d2430798cde398eb398300"
          }
        },
        "e4684b9b9a424941a5c83df21d6e1c41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_74773e62a3c049e7bab8d7f979bbafee",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3/3 [00:00&lt;00:00,  4.98it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9ccd5c815a3f499b987329e17b9603fa"
          }
        },
        "119a8b3205304f48a8df621273e8ee1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "73578e285ef64143b34077c0d02a3699": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9d0b0d4e7b5441858bf99294e07ff31c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a61f981195d2430798cde398eb398300": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "74773e62a3c049e7bab8d7f979bbafee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9ccd5c815a3f499b987329e17b9603fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skywalkerzhang/notes/blob/master/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets scipy scikit-learn torch torchvision torchaudio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvS6C4rvQ52g",
        "outputId": "ca18ac49-dc8d-44c3-fce5-c2d70558815d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.16.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.18.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (0.10.0+cu111)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.1.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.11)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load_data\n",
        "import dataclasses\n",
        "import json\n",
        "import logging\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Union\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import PreTrainedTokenizer\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class STSInputExample:\n",
        "    \"\"\"A single training/test example for semantic textual similarity.\n",
        "\n",
        "    Args:\n",
        "        guid: Unique id for the example.\n",
        "        text_a: string. The untokenized text of the first sequence. For single\n",
        "            sequence tasks, only this sequence must be specified.\n",
        "        text_b: string. The untokenized text of the second sequence.\n",
        "            Only must be specified for sequence pair tasks.\n",
        "        label: float. The label of the example.\n",
        "    \"\"\"\n",
        "\n",
        "    guid: str\n",
        "    text_a: str\n",
        "    text_b: str\n",
        "    label: float\n",
        "\n",
        "    def to_dict(self):\n",
        "        return dataclasses.asdict(self)\n",
        "\n",
        "    def to_json_string(self):\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2) + \"\\n\"\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class STSInputFeatures:\n",
        "    \"\"\"A single set of features of data. Property names are the same names as the corresponding inputs to a model.\n",
        "\n",
        "    Args:\n",
        "        input_ids: Indices of input sequence tokens in the vocabulary.\n",
        "        attention_mask: Mask to avoid performing attention on padding token indices.\n",
        "            Mask values selected in ``[0, 1]``: Usually ``1`` for tokens that are NOT MASKED, ``0`` for MASKED (padded)\n",
        "            tokens.\n",
        "        token_type_ids: (Optional) Segment token indices to indicate first and second\n",
        "            portions of the inputs. Only some models use them.\n",
        "        label: (Optional) Label corresponding to the input. Int for classification problems,\n",
        "            float for regression problems.\n",
        "    \"\"\"\n",
        "\n",
        "    input_ids: List[int]\n",
        "    attention_mask: Optional[List[int]] = None\n",
        "    token_type_ids: Optional[List[int]] = None\n",
        "    label: Optional[Union[int, float]] = None\n",
        "\n",
        "    def to_json_string(self):\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "        return json.dumps(dataclasses.asdict(self)) + \"\\n\"\n",
        "\n",
        "\n",
        "class STSDataset:\n",
        "    def __init__(self, data: list, tokenizer: PreTrainedTokenizer, max_seq_length: int):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.features = self._convert_features(self._create_examples(self.data))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        feature = self.features[idx]\n",
        "        input_ids = torch.tensor(feature.input_ids, dtype=torch.long)\n",
        "        attn_mask = torch.tensor(feature.attention_mask, dtype=torch.long)\n",
        "        token_type_ids = torch.tensor(\n",
        "            0 if feature.token_type_ids is None else feature.token_type_ids,\n",
        "            dtype=torch.long,\n",
        "        )\n",
        "        labels = torch.tensor(feature.label, dtype=torch.float)\n",
        "        return (input_ids, attn_mask, token_type_ids, labels)\n",
        "\n",
        "    def _create_examples(self, data):\n",
        "        examples = [\n",
        "            STSInputExample(\n",
        "                guid=d[\"idx\"],\n",
        "                text_a=d[\"sentence1\"],\n",
        "                text_b=d[\"sentence2\"],\n",
        "                label=d[\"label\"]\n",
        "            )\n",
        "            for d in self.data\n",
        "        ]\n",
        "        return examples\n",
        "\n",
        "    def _convert_features(\n",
        "        self, examples: List[STSInputExample]\n",
        "    ) -> List[STSInputFeatures]:\n",
        "        return convert_examples_to_features(\n",
        "            examples,\n",
        "            self.tokenizer,\n",
        "            max_length=self.max_seq_length,\n",
        "        )\n",
        "\n",
        "\n",
        "def convert_examples_to_features(\n",
        "    examples: List[STSInputExample],\n",
        "    tokenizer: PreTrainedTokenizer,\n",
        "    max_length: Optional[int] = None,\n",
        "):\n",
        "    if max_length is None:\n",
        "        max_length = tokenizer.model_max_length\n",
        "\n",
        "    labels = [float(example.label) for example in examples]\n",
        "\n",
        "    batch_encoding = tokenizer(\n",
        "        [(example.text_a, example.text_b) for example in examples],\n",
        "        max_length=max_length,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "    features = []\n",
        "    for i in range(len(examples)):\n",
        "        inputs = {k: batch_encoding[k][i] for k in batch_encoding}\n",
        "\n",
        "        feature = STSInputFeatures(**inputs, label=labels[i])\n",
        "        features.append(feature)\n",
        "\n",
        "    for i, example in enumerate(examples[:1]):\n",
        "        logger.info(\"*** Example ***\")\n",
        "        logger.info(\"guid: %s\" % (example.guid))\n",
        "        logger.info(\"features: %s\" % features[i])\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "class STSDataLoader(object):\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizer, max_length: Optional[int] = None):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length if max_length else self.tokenizer.model_max_length\n",
        "\n",
        "    def get_dataloader(self, data, batch_size, **kwargs):\n",
        "        dataset = STSDataset(data, self.tokenizer, self.max_length)\n",
        "        return DataLoader(dataset, batch_size=batch_size, shuffle=False, **kwargs)\n"
      ],
      "metadata": {
        "id": "vRSH7H76SXt7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer\n",
        "from datasets import Metric\n",
        "import torch\n",
        "from torch.optim import Optimizer\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import PreTrainedModel, PreTrainedTokenizer\n",
        "\n",
        "\n",
        "class Trainer(object):\n",
        "    def __init__(self,\n",
        "                 model: PreTrainedModel,\n",
        "                 tokenizer: PreTrainedTokenizer,\n",
        "                 train_loader: DataLoader,\n",
        "                 valid_loader: DataLoader,\n",
        "                 optimizer: Optimizer,\n",
        "                 scheduler: LambdaLR,\n",
        "                 metric: Metric,\n",
        "                 device: str,\n",
        "                 logger):\n",
        "\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.train_loader = train_loader\n",
        "        self.valid_loader = valid_loader\n",
        "\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "        self.metric = metric\n",
        "\n",
        "        self.device = device\n",
        "        self.logger = logger\n",
        "\n",
        "    def save_checkpoint(self, output_dir: str):\n",
        "        self.logger.info(\"saving model..\")\n",
        "        self.model.save_pretrained(output_dir)\n",
        "        self.tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    def optimize(self, loss: float):\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "        self.optimizer.step()\n",
        "        self.scheduler.step()\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "    def train(self, epoch: int, log_interval: int):\n",
        "        self.model.train()\n",
        "\n",
        "        train_loss = 0.0\n",
        "        n_data = 0\n",
        "        for i, batch in enumerate(self.train_loader, 1):\n",
        "            input_ids, attention_mask, token_type_ids, labels = [x.to(self.device) for x in batch]\n",
        "            output = self.model(\n",
        "                input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, labels=labels\n",
        "            )\n",
        "            loss = output['loss']\n",
        "            self.optimize(loss)\n",
        "\n",
        "            train_loss += loss * input_ids.size(0)\n",
        "            n_data += input_ids.size(0)\n",
        "\n",
        "            if i % log_interval == 0 or i == len(self.train_loader) - 1:\n",
        "                self.logger.info(f\"[Epoch {epoch+1} / Step {i}] \" + \"loss={:.4f}\".format(train_loss / n_data))\n",
        "                train_loss = 0.0\n",
        "                n_data = 0\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def eval(self, best_score: float, output_dir: str):\n",
        "        self.model.eval()\n",
        "\n",
        "        valid_loss = 0.0\n",
        "        for _, batch in enumerate(self.valid_loader, 1):\n",
        "            input_ids, attention_mask, token_type_ids, labels = [x.to(self.device) for x in batch]\n",
        "            output = self.model(\n",
        "                input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, labels=labels\n",
        "            )\n",
        "            valid_loss += output['loss'] * input_ids.size(0)\n",
        "            logits = output['logits']\n",
        "\n",
        "            self.metric.add_batch(predictions=logits, references=labels)\n",
        "\n",
        "        spearmanr = self.metric.compute()['spearmanr']\n",
        "        valid_loss /= len(self.valid_loader.dataset)\n",
        "\n",
        "        self.logger.info(\"[Valid] \" + \"loss={:.4f}  spearmanr={:.4f};\".format(valid_loss, spearmanr))\n",
        "        if spearmanr > best_score:\n",
        "            best_score = spearmanr\n",
        "            self.logger.info(\"Hit the best score\")\n",
        "            self.save_checkpoint(output_dir)\n",
        "        return best_score\n"
      ],
      "metadata": {
        "id": "L_5NoekCShJ4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# utils\n",
        "from datetime import datetime\n",
        "import json\n",
        "import os\n",
        "\n",
        "from pytz import timezone\n",
        "\n",
        "\n",
        "def read_json(file_path):\n",
        "    with open(file_path) as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "def kst(sec, what):\n",
        "    kst = datetime.now(timezone('Asia/Seoul'))\n",
        "    return kst.timetuple()\n",
        "\n",
        "\n",
        "def make_dirs(directory):\n",
        "    if not os.path.isdir(directory):\n",
        "        os.makedirs(directory, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "Tml8df1-Sktb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class MyArgs():\n",
        "#   def __init__(self):\n",
        "#     self.epochs = 10\n",
        "#     self.log_interval = 100\n",
        "#     self.lr = 5e-05\n",
        "#     self.model_dir = './model'\n",
        "#     self.max_length = 512\n",
        "#     self.batch_size = 8\n",
        "#     self.model_name_or_path = 'bert-base-uncased'\n",
        "#     self.seed = 1\n",
        "#     self.output_dir = './output'\n",
        "#     self.data_dir = '/data'\n",
        "#     self.train_batch_size = 8\n",
        "#     self.valid_batch_size = 8\n",
        "#     self.warmup_proportion = 0.1\n",
        "class MyArgs():\n",
        "  def __init__(self):\n",
        "    self.epochs = 10\n",
        "    self.log_interval = 100\n",
        "    self.lr = 5e-05\n",
        "    self.model_dir = './model'\n",
        "    self.max_length = 512\n",
        "    self.batch_size = 1\n",
        "    self.model_name_or_path = 'microsoft/deberta-base'\n",
        "    self.seed = 1\n",
        "    self.output_dir = './output'\n",
        "    self.data_dir = '/data'\n",
        "    self.train_batch_size = 1\n",
        "    self.valid_batch_size = 1\n",
        "    self.warmup_proportion = 0.1"
      ],
      "metadata": {
        "id": "YYb8N87lu0dT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onAZeRVs8aqM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195,
          "referenced_widgets": [
            "926d81bf5ac24f17bd84d7d6f954b006",
            "f191ced3ad0e420daabe7d1310eadc50",
            "b80d46f9e6ca44dab30b882afdb6b5f5",
            "46dfa8c0970a41e3ad02c405c28a3822",
            "e4684b9b9a424941a5c83df21d6e1c41",
            "119a8b3205304f48a8df621273e8ee1a",
            "73578e285ef64143b34077c0d02a3699",
            "9d0b0d4e7b5441858bf99294e07ff31c",
            "a61f981195d2430798cde398eb398300",
            "74773e62a3c049e7bab8d7f979bbafee",
            "9ccd5c815a3f499b987329e17b9603fa"
          ]
        },
        "outputId": "37b1e77d-c1fe-47a9-94c7-502510c354a6"
      },
      "source": [
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "\n",
        "from datasets import load_dataset, load_metric\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    AutoConfig,\n",
        "    # AutoModelForSequenceClassification,\n",
        "    # AutoTokenizer,\n",
        "    DebertaTokenizer, \n",
        "    DebertaForSequenceClassification,\n",
        "    get_linear_schedule_with_warmup,\n",
        "    set_seed,\n",
        ")\n",
        "import torch\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "\n",
        "def build_adamw_optimizer(lr, model, num_train_steps, num_warmup_steps, global_step=0):\n",
        "    last_epoch = -1 if global_step == 0 else global_step\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n",
        "    for group in optimizer.param_groups:\n",
        "        group['initial_lr'] = lr\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps=num_warmup_steps,\n",
        "                                                num_training_steps=num_train_steps,\n",
        "                                                last_epoch=last_epoch)\n",
        "    return optimizer, scheduler\n",
        "\n",
        "\n",
        "def train(args):\n",
        "    # Set logger & seed\n",
        "    set_seed(args.seed)\n",
        "    make_dirs(args.model_dir)\n",
        "    logging.Formatter.converter = kst\n",
        "    logging.basicConfig(filename=os.path.join(args.model_dir, 'logs_train.txt'),\n",
        "                        filemode='w', format='%(asctime)s -  %(message)s',\n",
        "                        datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO)\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logger.info(args)\n",
        "\n",
        "    # Set device\n",
        "    num_gpus = torch.cuda.device_count()\n",
        "    use_cuda = num_gpus > 0\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    logger.info(f\"***** using {device} *****\")\n",
        "    logger.info(f\"***** num GPU: {num_gpus} *****\")\n",
        "\n",
        "    # Build PLM config & tokenizer\n",
        "    config = AutoConfig.from_pretrained(\n",
        "        args.model_name_or_path,\n",
        "        num_labels=1,\n",
        "        finetuning_task=\"stsb\"\n",
        "    )\n",
        "    # tokenizer = AutoTokenizer.from_pretrained(\n",
        "    tokenizer = DebertaTokenizer.from_pretrained(\n",
        "        args.model_name_or_path,\n",
        "        use_fast=True,\n",
        "    )\n",
        "\n",
        "    # Build data loader\n",
        "    datasets = load_dataset(\"glue\", \"stsb\")\n",
        "    sts_dataloader = STSDataLoader(tokenizer)\n",
        "    train_loader = sts_dataloader.get_dataloader(\n",
        "        data=list(datasets['train']),\n",
        "        batch_size=args.train_batch_size,\n",
        "    )\n",
        "    valid_loader = sts_dataloader.get_dataloader(\n",
        "        data=list(datasets['validation']),\n",
        "        batch_size=args.valid_batch_size,\n",
        "    )\n",
        "\n",
        "    # Build model\n",
        "    # model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model = DebertaForSequenceClassification.from_pretrained(\n",
        "        args.model_name_or_path,\n",
        "        config=config,\n",
        "    )\n",
        "    if num_gpus > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Build optimizer\n",
        "    total_train_steps = len(train_loader) * args.epochs\n",
        "    num_warmup_steps = int(args.warmup_proportion * total_train_steps)\n",
        "    optimizer, scheduler = build_adamw_optimizer(args.lr,\n",
        "                                                 model,\n",
        "                                                 total_train_steps,\n",
        "                                                 num_warmup_steps)\n",
        "\n",
        "    # Set metric\n",
        "    metric = load_metric(\"glue\", \"stsb\")\n",
        "\n",
        "    # Build trainer\n",
        "    trainer = Trainer(model=model,\n",
        "                      tokenizer=tokenizer,\n",
        "                      train_loader=train_loader,\n",
        "                      valid_loader=valid_loader,\n",
        "                      optimizer=optimizer,\n",
        "                      scheduler=scheduler,\n",
        "                      metric=metric,\n",
        "                      device=device,\n",
        "                      logger=logger)\n",
        "\n",
        "    logger.info(\"***** training start *****\")\n",
        "    logger.info(\"Learning rate: \" + f\"{args.lr}\")\n",
        "    logger.info(f\"Batch_size : {args.train_batch_size * max(num_gpus, 1)}\")\n",
        "\n",
        "    best_score = 0.0\n",
        "    for epoch in range(args.epochs):\n",
        "        trainer.train(epoch, args.log_interval)\n",
        "        best_score = trainer.eval(best_score, args.model_dir)\n",
        "\n",
        "\n",
        "# parser = argparse.ArgumentParser()\n",
        "# parser.add_argument(\n",
        "#     \"--model_name_or_path\", type=str, default='bert-base-uncased',\n",
        "#     help='Path to pretrained model or model name from huggingface'\n",
        "# )\n",
        "# parser.add_argument(\n",
        "#     '--train_batch_size', type=int, default=1,\n",
        "#     help='input batch size for training'\n",
        "# )\n",
        "# parser.add_argument(\n",
        "#     '--valid_batch_size', type=int, default=1,\n",
        "#     help='input batch size for validing'\n",
        "# )\n",
        "# parser.add_argument(\n",
        "#     '--epochs', type=int, default=10,\n",
        "#     help='number of epochs to train'\n",
        "# )\n",
        "# parser.add_argument(\n",
        "#     '--lr', type=float, default=5e-5,\n",
        "#     help='learning rate'\n",
        "# )\n",
        "# parser.add_argument(\n",
        "#     '--warmup_proportion', type=float, default=0.1,\n",
        "#     help=\"Proportion of lr increasing steps\"\n",
        "# )\n",
        "# parser.add_argument(\n",
        "#     '--seed', type=int, default=1,\n",
        "#     help='random seed'\n",
        "# )\n",
        "# parser.add_argument(\n",
        "#     '--log_interval', type=int, default=100,\n",
        "#     help='how many batches to wait before logging training status'\n",
        "# )\n",
        "\n",
        "# # Container environment\n",
        "# parser.add_argument(\n",
        "#     \"--model_dir\", type=str, default=os.environ.get('SM_MODEL_DIR', './model'),\n",
        "#     help='path to save output'\n",
        "# )\n",
        "# args = parser.parse_args()\n",
        "\n",
        "\n",
        "train(MyArgs())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "926d81bf5ac24f17bd84d7d6f954b006",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias']\n",
            "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'pooler.dense.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "\n",
        "from transformers import DebertaTokenizer, DebertaForSequenceClassification\n",
        "import torch\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def inference(args):\n",
        "    # Set device\n",
        "    num_gpus = torch.cuda.device_count()\n",
        "    use_cuda = num_gpus > 0\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    # Load model\n",
        "    model = DebertaForSequenceClassification.from_pretrained(args.model_dir)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = DebertaTokenizer.from_pretrained(args.model_dir)\n",
        "\n",
        "    # Build data loader\n",
        "    sts_dataloader = STSDataLoader(tokenizer, args.max_length)\n",
        "    # test_data = read_json(os.path.join(args.data_dir, 'sts_test.json'))\n",
        "    datasets = load_dataset(\"glue\", \"stsb\")\n",
        "    test_data = datasets['test']\n",
        "    sts_test_loader = sts_dataloader.get_dataloader(\n",
        "        data=test_data,\n",
        "        batch_size=args.batch_size,\n",
        "    )\n",
        "\n",
        "    # Infer\n",
        "    make_dirs(args.output_dir)\n",
        "    output_file = open(os.path.join(args.output_dir, \"output.csv\"), \"w\")\n",
        "    for out in sts_test_loader:\n",
        "        input_ids, attention_mask, token_type_ids, _ = [o.to(device) for o in out]\n",
        "        output = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)[0]\n",
        "\n",
        "        preds = output.detach().cpu().numpy()\n",
        "\n",
        "        for p in preds:\n",
        "            score = p[0]\n",
        "            output_file.write(f\"{score}\\n\")\n",
        "\n",
        "    output_file.close()\n",
        "\n",
        "\n",
        "def main():\n",
        "    # parser = argparse.ArgumentParser()\n",
        "\n",
        "    # parser.add_argument(\n",
        "    #     \"--batch_size\",\n",
        "    #     type=int,\n",
        "    #     default=32,\n",
        "    #     metavar=\"N\",\n",
        "    #     help=\"input batch size for inference (default: 64)\",\n",
        "    # )\n",
        "\n",
        "    # parser.add_argument(\n",
        "    #     \"--data_dir\", type=str, default=os.environ.get(\"SM_CHANNEL_EVAL\", \"./data\"),\n",
        "    #     help='path to load test data'\n",
        "    # )\n",
        "    # parser.add_argument(\n",
        "    #     \"--model_dir\", type=str, default=os.environ.get(\"SM_CHANNEL_MODEL\", \"./model\"),\n",
        "    #     help='path to load trained model'\n",
        "    # )\n",
        "    # parser.add_argument(\n",
        "    #     \"--output_dir\", type=str, default=os.environ.get(\"SM_OUTPUT_DATA_DIR\", \"./output\"),\n",
        "    #     help='path to save the output'\n",
        "    # )\n",
        "    # parser.add_argument(\n",
        "    #     \"--max_length\",\n",
        "    #     type=int,\n",
        "    #     default=512,\n",
        "    #     help=\"maximum sequence length\",\n",
        "    # )\n",
        "\n",
        "    # args = parser.parse_args()\n",
        "\n",
        "    inference(MyArgs())\n",
        "\n",
        "\n",
        "main()\n"
      ],
      "metadata": {
        "id": "nZp1-Ew2tVi6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}